"use client";

import { useState, useEffect } from "react";
import { Button } from "@/components/ui/button";
import { ArrowRightIcon } from "lucide-react";
import Image from "next/image";

import { useTransformerSimulation } from "./transformer-simulation";

interface StepContent {
  title: string;
  detailedExplanation: string;
  keyPoints: string[];
}



const stepContentData: StepContent[] = [
  {
    title: "Input Tokens",
    detailedExplanation:
      "Tokenization is the first step where raw text is broken down into smaller units called tokens. These tokens can be words, subwords, or characters depending on the tokenizer used. For example, 'Hello world' might become ['Hello', 'world'] or ['Hel', '##lo', 'world'] with subword tokenization.",
    keyPoints: [
      "Tokens are the basic units of text processing",
      "Different tokenizers use different strategies (word, subword, character)",
      "Each token gets a unique ID from the vocabulary",
      "Special tokens like [CLS] and [SEP] are added for structure",
    ],
  },
  {
    title: "Embeddings",
    detailedExplanation:
      "Token embeddings convert discrete token IDs into continuous vector representations. Each token is mapped to a high-dimensional vector (e.g., 512 or 768 dimensions) that captures semantic meaning. Similar words have similar embeddings, allowing the model to understand relationships.",
    keyPoints: [
      "Embeddings are learned during training",
      "Each dimension captures different semantic features",
      "Similar tokens have vectors close together in embedding space",
      "Position information is added via positional encodings",
    ],
    quiz: [
      {
        question: "What do embeddings represent?",
        options: [
          "Token colors",
          "Vector representations of tokens",
          "Token frequencies",
          "Token lengths",
        ],
        correctAnswer: 1,
        explanation:
          "Embeddings are vector representations that capture the semantic meaning of tokens.",
      },
      {
        question: "Why are embeddings useful?",
        options: [
          "They make tokens smaller",
          "They capture semantic relationships",
          "They speed up processing",
          "They reduce memory usage",
        ],
        correctAnswer: 1,
        explanation:
          "Embeddings capture semantic relationships, allowing similar words to have similar vector representations.",
      },
      {
        question: "What is the typical dimensionality of embeddings in transformers?",
        options: [
          "10-50 dimensions",
          "512-768 dimensions",
          "5000-10000 dimensions",
          "Only 3 dimensions",
        ],
        correctAnswer: 1,
        explanation:
          "Modern transformers like GPT and BERT use embeddings with 512-768 dimensions (or even higher for larger models).",
      },
      {
        question: "What's special about similar words in embedding space?",
        options: [
          "They have the same color",
          "Their vectors are close together (high cosine similarity)",
          "They always appear next to each other in text",
          "They have the same length",
        ],
        correctAnswer: 1,
        explanation:
          "Words with similar meanings have similar embedding vectors, making them close in the high-dimensional space.",
      },
      {
        question: "How are embeddings initially created?",
        options: [
          "Manually by programmers",
          "Randomly initialized and learned during training",
          "Copied from a dictionary",
          "Generated by users",
        ],
        correctAnswer: 1,
        explanation:
          "Embeddings start as random vectors and are adjusted during training to capture meaningful relationships.",
      },
    ],
  },
  {
    title: "QKV Projection",
    detailedExplanation:
      "The QKV projection creates three different representations of each token: Query (Q), Key (K), and Value (V). These are created by multiplying the input embeddings with learned weight matrices. Queries ask 'what am I looking for?', Keys answer 'what do I contain?', and Values provide 'what information should be passed forward?'",
    keyPoints: [
      "Q, K, V are linear projections of the input",
      "Query: what information to look for",
      "Key: what information is available",
      "Value: the actual information to pass",
    ],
    quiz: [
      {
        question: "What does the Query (Q) represent?",
        options: [
          "The answer to a question",
          "What information a token is looking for",
          "The final output",
          "The input text",
        ],
        correctAnswer: 1,
        explanation:
          "The Query represents what information each token is searching for in other tokens.",
      },
      {
        question: "How are Q, K, V created?",
        options: [
          "Random generation",
          "Linear projection of inputs",
          "User input",
          "Copy of embeddings",
        ],
        correctAnswer: 1,
        explanation:
          "Q, K, and V are created by multiplying input embeddings with learned weight matrices.",
      },
    ],
  },
  {
    title: "Attention Mechanism",
    detailedExplanation:
      "The attention mechanism computes how much each token should 'attend to' (focus on) every other token. It calculates scores by comparing Queries with Keys, normalizes with softmax, and uses these scores to weight the Values. This allows the model to learn which tokens are relevant to each other.",
    keyPoints: [
      "Attention scores = softmax(Q·K^T / √d)",
      "Each token can attend to all other tokens",
      "Multiple attention heads capture different relationships",
      "Self-attention allows tokens to gather context",
    ],
    quiz: [
      {
        question: "What does attention calculate?",
        options: [
          "Token size",
          "Token relevance/importance to other tokens",
          "Token speed",
          "Token color",
        ],
        correctAnswer: 1,
        explanation:
          "Attention calculates how relevant each token is to every other token in the sequence.",
      },
      {
        question: "Why divide by √d in attention?",
        options: [
          "To make it faster",
          "To stabilize gradients and prevent extreme values",
          "To add randomness",
          "To reduce memory",
        ],
        correctAnswer: 1,
        explanation:
          "Dividing by √d (where d is dimension) prevents dot products from becoming too large, stabilizing training.",
      },
    ],
  },
  {
    title: "Feed-Forward Network",
    detailedExplanation:
      "After attention, each token passes through a feed-forward network (FFN) independently. The FFN consists of two linear layers with a non-linear activation (like ReLU or GELU) in between. This allows the model to process and transform the information gathered from attention.",
    keyPoints: [
      "Applied to each position independently",
      "Typically: Linear → Activation → Linear",
      "Expands dimension then contracts back",
      "Adds non-linearity and processing power",
    ],
    quiz: [
      {
        question: "What is unique about FFN processing?",
        options: [
          "It processes all tokens together",
          "It processes each token independently",
          "It only processes the first token",
          "It skips some tokens",
        ],
        correctAnswer: 1,
        explanation:
          "The FFN processes each token independently, unlike attention which looks at relationships between tokens.",
      },
      {
        question: "What does the FFN add to the model?",
        options: [
          "More tokens",
          "Non-linearity and transformation capacity",
          "Random noise",
          "Attention scores",
        ],
        correctAnswer: 1,
        explanation:
          "The FFN adds non-linear transformations, allowing the model to learn complex patterns beyond linear relationships.",
      },
    ],
  },
  {
    title: "Layer Normalization",
    detailedExplanation:
      "Layer normalization stabilizes training by normalizing the values across features for each token. It computes the mean and variance and scales the values to have zero mean and unit variance. This prevents values from exploding or vanishing during training and speeds up convergence.",
    keyPoints: [
      "Normalizes across feature dimension",
      "Prevents gradient explosion/vanishing",
      "Speeds up training convergence",
      "Applied after attention and FFN",
    ],
    quiz: [
      {
        question: "What is the main benefit of layer normalization?",
        options: [
          "Makes the model faster",
          "Stabilizes training and prevents extreme values",
          "Reduces model size",
          "Adds more layers",
        ],
        correctAnswer: 1,
        explanation:
          "Layer normalization stabilizes training by keeping values in a reasonable range, preventing gradient issues.",
      },
      {
        question: "When is layer normalization applied?",
        options: [
          "Only at the start",
          "After attention and FFN sublayers",
          "Only at the end",
          "Never",
        ],
        correctAnswer: 1,
        explanation:
          "Layer normalization is applied after both the attention and feed-forward network sublayers.",
      },
    ],
  },
  {
    title: "Output Hidden States",
    detailedExplanation:
      "After passing through all transformer layers (attention + FFN + normalization), we get the final hidden states. These are rich, contextualized representations of each token that incorporate information from the entire sequence. Each token's hidden state now contains understanding of its role and context.",
    keyPoints: [
      "Final contextual representation of each token",
      "Incorporates information from entire sequence",
      "Used for downstream tasks (classification, generation, etc.)",
      "Stacking layers increases representation power",
    ],
    quiz: [
      {
        question: "What information do final hidden states contain?",
        options: [
          "Only the original token",
          "Contextualized information from the entire sequence",
          "Random values",
          "Only neighboring tokens",
        ],
        correctAnswer: 1,
        explanation:
          "Final hidden states are contextualized representations that incorporate information from all tokens in the sequence.",
      },
      {
        question: "What makes deeper transformers more powerful?",
        options: [
          "They run faster",
          "Each layer adds more contextual understanding",
          "They use less memory",
          "They need less training",
        ],
        correctAnswer: 1,
        explanation:
          "Each transformer layer adds another level of contextual understanding and abstraction.",
      },
    ],
  },
  {
    title: "Softmax & Prediction",
    detailedExplanation:
      "The final step converts hidden states into probability distributions over the vocabulary using softmax. For each position, the model outputs probabilities for what token should come next. The token with the highest probability is typically selected (or sampled from the distribution for creative generation).",
    keyPoints: [
      "Converts logits to probability distribution",
      "Sum of all probabilities = 1",
      "Highest probability token is most likely next token",
      "Temperature parameter controls randomness",
    ],
    quiz: [
      {
        question: "What does softmax do?",
        options: [
          "Makes values soft",
          "Converts scores to probability distribution",
          "Deletes tokens",
          "Adds noise",
        ],
        correctAnswer: 1,
        explanation:
          "Softmax converts raw scores (logits) into a probability distribution where all values sum to 1.",
      },
      {
        question: "How does temperature affect generation?",
        options: [
          "Higher = more random/creative",
          "Higher = more deterministic",
          "No effect",
          "Only affects speed",
        ],
        correctAnswer: 0,
        explanation:
          "Higher temperature makes the distribution more uniform (more random), while lower temperature makes it more peaked (more deterministic).",
      },
    ],
  },
];

export function SimpleAITutor() {
  // Get values from transformer simulation context
  const context = useTransformerSimulation();
  
  if (!context) {
    throw new Error("SimpleAITutor must be used within TransformerSimulationProvider");
  }
  
  const { 
    currentStep, 
    nextStep: onNext, 
    autoContinue, 
    setAutoContinue,
    inputText,
    setInputText,
    predictedToken,
  } = context;
  
  const [explanation, setExplanation] = useState<string>("");
  const [isLoading, setIsLoading] = useState(false);
  const [hasStarted, setHasStarted] = useState(false);
  const [currentStepContent, setCurrentStepContent] = useState<StepContent>(stepContentData[0]);


  // Load step content statically
  useEffect(() => {
    if (hasStarted) {
      setCurrentStepContent(stepContentData[currentStep]);
    }
  }, [currentStep, hasStarted]);

  const stepPrompts = [
    "We're looking at Step 0: Input Tokens. In your own words, explain what's happening here - how does text get split into tokens? Keep it casual and friendly.",
    "Now we're at Step 1: Embeddings. What's going on as these tokens transform into vectors? Make it relatable.",
    "Step 2: QKV Projection. This is where things get interesting - what are Query, Key, and Value doing here? Explain like you're chatting with a friend.",
    "Step 3: Attention - this is the transformer's superpower! How do tokens learn to pay attention to each other? Get us excited about it.",
    "Step 4: Feed-Forward Network. After all that attention, what happens in the FFN? Why does each token need this processing?",
    "Step 5: Layer Normalization. Things could get messy without this - what's it doing to keep everything stable?",
    "Step 6: Output. We're almost there! What are these final hidden states, and why do they matter?",
    "Step 7: Softmax. The grand finale - how does the model predict what comes next? Make it click for us.",
  ];

  const getExplanation = async (step: number) => {
    setIsLoading(true);
    try {
      const response = await fetch("/api/ai/simulate-transformer", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          prompts: [{ 
            role: "user", 
            content: `${stepPrompts[step]}\n\nContext: The current input text is "${inputText}". Reference this input in your explanation when relevant.` 
          }],
          tools: [],
        }),
      });

      const data = await response.json();
      setExplanation(data.content || "Let's continue exploring transformers!");
    } catch (error) {
      console.error("Error:", error);
      setExplanation("Let's continue to the next step!");
    } finally {
      setIsLoading(false);
    }
  };

  const handleStart = async () => {
    setIsLoading(true);
    // const suggested = await getSuggestedInput();
    // setInputText(suggested);
    setHasStarted(true);
    await getExplanation(0);
    setIsLoading(false);
  };

  const handleContinue = () => {
    if (currentStep < 7) {
      onNext();
      getExplanation(currentStep + 1);
    }
  };

  if (!hasStarted) {
    return (
      <AIExplanation
        text="I'm Mela! Ready to see how transformers work? I'll pick a simple phrase to demonstrate, then we'll walk through all 8 steps together. Click when you're ready!"
        position={{ x: -40, y: 450 }}
        autohide={false}
      >
        <Button
          className="cursor-pointer"
          variant="secondary"
          size="icon"
          onClick={handleStart}
          disabled={isLoading}
        >
          {isLoading ? "..." : <ArrowRightIcon />}
        </Button>
      </AIExplanation>
    );
  }

  if (explanation && !isLoading) {
    return (
      <AIExplanation
        text={explanation}
        position={{ x: -40, y: 300 }}
        autohide={false}
      >
        <Button
          className="cursor-pointer"
          variant="secondary"
          size="icon"
          onClick={handleContinue}
          disabled={isLoading}
        >
          <ArrowRightIcon />
        </Button>
      </AIExplanation>
    );
  }

  return null;
}







export const AIExplanation = ({
  text,
  position,
  children,
  autohide = true,
}: {
  text: string;
  autohide: boolean;
  position: { x: number; y: number };
  children: React.ReactNode;
}) => {
  const [isVisible, setIsVisible] = useState(true);

  return (
    <div
      style={{
        position: "absolute",
        left: position.x,
        top: position.y,
        opacity: isVisible ? 1 : 0,
        transform: isVisible
          ? "translateY(0) scale(1)"
          : "translateY(20px) scale(0.95)",
        transition: "all 0.5s cubic-bezier(0.34, 1.56, 0.64, 1)",
        pointerEvents: isVisible ? "auto" : "none",
      }}
    >
      <div className="relative flex items-start gap-2">
        <Image
          src="/mela.webp"
          width={200}
          height={400}
          draggable={false}
          alt="AI Explanation"
          className="rounded-2xl rounded-tr-none object-cover object-top select-none"
        />
        <div className="flex flex-col items-end justify-end gap-2 -ml-8">
          <div className="bg-white rounded-2xl rounded-bl-none shadow-lg p-4 max-w-[20rem]">
            <p className="text-sm text-gray-800">{text}</p>
          </div>
          {children}
        </div>
      </div>
    </div>
  );
};
